{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e070b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from models.resnet import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d2cf0",
   "metadata": {},
   "source": [
    "#### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_directory(src_dir = \"./data/test\", dst_root = \"./data/test\"):\n",
    "    os.makedirs(dst_root, exist_ok=True)\n",
    "    img_exts = {'.jpg'}\n",
    "\n",
    "    for filename in os.listdir(src_dir):\n",
    "        if not any(filename.lower().endswith(ext) for ext in img_exts):\n",
    "            continue \n",
    "        prefix = filename.split('_')[0]\n",
    "        target_dir = os.path.join(dst_root, prefix)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "        src_path = os.path.join(src_dir, filename)\n",
    "        dst_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        shutil.move(src_path, dst_path)\n",
    "    print(\"directory structure created\")\n",
    "\n",
    "create_dataset_directory(src_dir = \"./data/test\", dst_root = \"./data/test\")\n",
    "create_dataset_directory(src_dir = \"./data/train\", dst_root = \"./data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the test dataset size. \n",
    "def move_into_test(test_dir = \"data/test\", train_dir = \"data/train\"):\n",
    "    if not os.path.exists(train_dir) or not os.path.exists(test_dir):\n",
    "        raise FileNotFoundError(\"Ensure both 'data/train' and 'data/test' directories exist.\")\n",
    "\n",
    "    for class_name in os.listdir(train_dir):\n",
    "        train_class_path = os.path.join(train_dir, class_name)\n",
    "        test_class_path = os.path.join(test_dir, class_name)\n",
    "\n",
    "        if not os.path.isdir(train_class_path):\n",
    "            continue  \n",
    "\n",
    "        if not os.path.exists(test_class_path):\n",
    "            os.makedirs(test_class_path)  # Create target class folder if missing\n",
    "\n",
    "        files = os.listdir(train_class_path)\n",
    "        files = [f for f in files if os.path.isfile(os.path.join(train_class_path, f))]\n",
    "\n",
    "        if len(files) < 300:\n",
    "            print(f\"Skipping {class_name}: not enough files to move.\")\n",
    "            continue\n",
    "\n",
    "        files_to_move = random.sample(files, 300)\n",
    "\n",
    "        for file_name in files_to_move:\n",
    "            src_path = os.path.join(train_class_path, file_name)\n",
    "            dst_path = os.path.join(test_class_path, file_name)\n",
    "            shutil.move(src_path, dst_path)\n",
    "        print(f\"moved 300 files from '{class_name}' train to test.\")\n",
    "\n",
    "# run this code only once\n",
    "# move_into_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_train = './data/train'\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "root_test = './data/test'\n",
    "test_transform = transforms.Compose([\n",
    "transforms.Resize((224, 224)),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_set = datasets.ImageFolder(root_test, transform = test_transform)\n",
    "train_dataset = datasets.ImageFolder(root_train, transform = train_transform)\n",
    "\n",
    "\n",
    "train_size = int(0.80 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_set, val_set= random_split(train_dataset, [train_size, val_size])\n",
    "val_set.dataset.transform = test_transform # change transforms for val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0672b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_set, batch_size=1024)\n",
    "test_loader = DataLoader(test_set, batch_size=1024)\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0606966",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, checkpoint):\n",
    "    torch.save(model.state_dict(), checkpoint)\n",
    "\n",
    "def validate(model, val_loader, loss_fn = CrossEntropyLoss()):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        N = 0\n",
    "        mean_loss = 0\n",
    "    \n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "\n",
    "            N += len(x)\n",
    "            mean_loss += len(x) * (loss.item() - mean_loss)/N         \n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      total = 0\n",
    "      correct = 0\n",
    "      for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        prob = torch.softmax(output, dim = 1)\n",
    "        pred = prob.max(dim = 1, keepdim = True)[1]\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "        total += len(y)\n",
    "    acc = correct/total\n",
    "    return acc\n",
    "\n",
    "def plot_cm(model, test_loader, title):\n",
    "    # Collect predictions and true labels for the test set\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def calculate_precision_recall(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    precision = np.sum(all_preds[all_labels == 1] == 1) / np.sum(all_preds == 1)\n",
    "    recall = np.sum(all_preds[all_labels == 1] == 1) / np.sum(all_labels == 1)\n",
    "\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a1c09",
   "metadata": {},
   "source": [
    "#### Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, checkpoint, learning_rate = 0.01, epochs=20):\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # scheduler = MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
    "    val_loss_list = []\n",
    "\n",
    "    #acc and loss before training\n",
    "    val_loss = validate(model, val_loader)\n",
    "    val_loss_list.append(val_loss)\n",
    "    best_loss = val_loss\n",
    "    best_model = deepcopy(model)\n",
    "    print(f\"validation loss prior to training: {val_loss:.4f}\")\n",
    "    \n",
    "    for epoch in tqdm(range(0, epochs)):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # scheduler.step()\n",
    "        #acc and loss \n",
    "        model.eval()\n",
    "        val_loss = validate(model, val_loader)\n",
    "\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = deepcopy(model)\n",
    "            save_checkpoint(model, checkpoint)\n",
    "        print(f\"val loss : {val_loss:.4f}\")\n",
    "\n",
    "    return best_model, val_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cfc15",
   "metadata": {},
   "source": [
    "#### Experiment with ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18(num_classes = 29, pretrained=False):\n",
    "    model = resnet18(pretrained)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "model = get_resnet18(num_classes=29, pretrained=False)\n",
    "model_path = 'resnet18.pth' \n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    model, val_loss_list = train(model, train_loader, val_loader, 'resnet18.pth')\n",
    "    np.save('val_loss.npy', val_loss_list)\n",
    "    save_checkpoint(model, 'resnet18.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936d387",
   "metadata": {},
   "source": [
    "#### Visualize loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list = np.load('resnet18_val_loss.npy')\n",
    "plt.plot(val_loss_list, label='val Loss')\n",
    "plt.legend()\n",
    "plt.title(\"training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005ca16",
   "metadata": {},
   "source": [
    "#### visualize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, layer, image_tensor):\n",
    "    def hook_fn(module, input, output):\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        for i in range(min(64, output.size(1))):\n",
    "            plt.subplot(8, 8, i + 1)\n",
    "            plt.imshow(output[0, i].detach().cpu().numpy(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(\"Feature Maps\")\n",
    "        plt.show()\n",
    "\n",
    "    handle = layer.register_forward_hook(hook_fn)\n",
    "    _ = model(image_tensor.unsqueeze(0).to(device))\n",
    "    handle.remove()\n",
    "\n",
    "image, _ = test_set[0]\n",
    "visualize_feature_maps(model, model.conv1, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939cf475",
   "metadata": {},
   "source": [
    "Calculate Precision/Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1396d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision, recall = calculate_precision_recall(model, test_loader)\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdd5f4",
   "metadata": {},
   "source": [
    "Plot confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ada86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(model, test_loader, title = \"Resnet18 Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a0718",
   "metadata": {},
   "source": [
    "#### evaluate trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5811c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy(model, test_loader)\n",
    "print(f\"test accuracy of trained model is: {test_accuracy*100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e54bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = accuracy(model, val_loader)\n",
    "print(f\"val accuracy of trained model is: {val_accuracy*100:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6c4c1",
   "metadata": {},
   "source": [
    "#### Experiment with DenseNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_densenet121(num_classes = 29, pretrained=False):\n",
    "    model = models.densenet121(pretrained=pretrained)\n",
    "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "model = get_densenet121(num_classes=29, pretrained=True)\n",
    "model_path = 'densenet121.pth' \n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "else:\n",
    "    model, val_loss_list = train(model, train_loader, val_loader, model_path)\n",
    "    np.save('densenet121_val_loss.npy', val_loss_list)\n",
    "    save_checkpoint(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e93764",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list = np.load('densenet121_val_loss.npy')\n",
    "plt.plot(val_loss_list, label='val Loss')\n",
    "plt.legend()\n",
    "plt.title(\"training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76508301",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _ = test_set[0]\n",
    "visualize_feature_maps(model, model.features.denseblock1.denselayer1.conv1, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy(model, test_loader)\n",
    "print(f\"test accuracy of trained model is: {test_accuracy*100:.3f}\")\n",
    "val_accuracy = accuracy(model, val_loader)\n",
    "print(f\"val accuracy of trained model is: {val_accuracy*100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d841d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "plot_cm(model, test_loader, title = \"Densenet121 Confusion Matrix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
